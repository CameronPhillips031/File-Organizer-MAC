 (cd "$(git rev-parse --show-toplevel)" && git apply --3way <<'EOF' 
diff --git a/src/cratesai_pro/analysis/analyzer.py b/src/cratesai_pro/analysis/analyzer.py
new file mode 100644
index 0000000000000000000000000000000000000000..18ff37a110ee04943904a441c85476b3a53673fc
--- /dev/null
+++ b/src/cratesai_pro/analysis/analyzer.py
@@ -0,0 +1,181 @@
+"""Audio analysis utilities for CratesAI Pro."""
+from __future__ import annotations
+
+from dataclasses import dataclass
+from pathlib import Path
+from typing import List, Sequence
+
+import librosa
+import numpy as np
+import pyloudnorm as pyln
+
+from ..models import CuePoint, TrackAnalysis
+
+SUPPORTED_EXTENSIONS: Sequence[str] = (".mp3", ".m4a", ".aac", ".wav", ".flac", ".aiff", ".ogg")
+
+_CAMELOT_MAP = {
+    ("C", "major"): "8B",
+    ("C#", "major"): "3B",
+    ("D", "major"): "10B",
+    ("Eb", "major"): "5B",
+    ("E", "major"): "12B",
+    ("F", "major"): "7B",
+    ("F#", "major"): "2B",
+    ("G", "major"): "9B",
+    ("Ab", "major"): "4B",
+    ("A", "major"): "11B",
+    ("Bb", "major"): "6B",
+    ("B", "major"): "1B",
+    ("C", "minor"): "5A",
+    ("C#", "minor"): "12A",
+    ("D", "minor"): "7A",
+    ("Eb", "minor"): "2A",
+    ("E", "minor"): "9A",
+    ("F", "minor"): "4A",
+    ("F#", "minor"): "11A",
+    ("G", "minor"): "6A",
+    ("Ab", "minor"): "1A",
+    ("A", "minor"): "8A",
+    ("Bb", "minor"): "3A",
+    ("B", "minor"): "10A",
+}
+
+_NOTE_NAMES = np.array(["C", "C#", "D", "Eb", "E", "F", "F#", "G", "Ab", "A", "Bb", "B"])
+_MAJOR_PROFILE = np.array([6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88], dtype=float)
+_MINOR_PROFILE = np.array([6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17], dtype=float)
+
+
+@dataclass(slots=True)
+class AnalyzerConfig:
+    """Configuration values that control analysis behaviour."""
+
+    sample_rate: int = 22050
+    hop_length: int = 512
+    bpm_floor: float = 70
+    bpm_ceiling: float = 200
+
+
+class AnalysisEngine:
+    """Perform DSP-heavy analysis over audio files."""
+
+    def __init__(self, config: AnalyzerConfig | None = None) -> None:
+        self.config = config or AnalyzerConfig()
+        self._meter = pyln.Meter(self.config.sample_rate)
+
+    def discover_tracks(self, root: Path) -> List[Path]:
+        """Return all supported audio files within ``root``."""
+
+        tracks: List[Path] = []
+        for path in root.rglob("*"):
+            if path.is_file() and path.suffix.lower() in SUPPORTED_EXTENSIONS:
+                tracks.append(path)
+        tracks.sort()
+        return tracks
+
+    def analyze(self, path: Path) -> TrackAnalysis:
+        """Analyze a single audio file and return structured metadata."""
+
+        if not path.exists():
+            raise FileNotFoundError(path)
+
+        audio, sr = librosa.load(path, sr=self.config.sample_rate, mono=True)
+        duration = float(librosa.get_duration(y=audio, sr=sr))
+
+        bpm = self._estimate_bpm(audio, sr)
+        key, mode = self._estimate_key(audio, sr)
+        camelot = _CAMELOT_MAP.get((key, mode), "?")
+        energy = self._estimate_energy(audio)
+        cues = self._detect_cues(audio, sr)
+        title, artist, album, genre = _extract_basic_tags(path)
+
+        return TrackAnalysis(
+            source_path=path,
+            title=title,
+            artist=artist,
+            album=album,
+            genre=genre,
+            bpm=bpm,
+            key=f"{key} {mode.title()}",
+            camelot=camelot,
+            energy=energy,
+            duration=duration,
+            cue_points=cues,
+        )
+
+    def _estimate_bpm(self, audio: np.ndarray, sample_rate: int) -> float:
+        tempo, _ = librosa.beat.beat_track(y=audio, sr=sample_rate, hop_length=self.config.hop_length)
+        bpm = float(tempo)
+        if bpm < self.config.bpm_floor:
+            bpm *= 2
+        elif bpm > self.config.bpm_ceiling:
+            bpm /= 2
+        return bpm
+
+    def _estimate_key(self, audio: np.ndarray, sample_rate: int) -> tuple[str, str]:
+        chroma = librosa.feature.chroma_cqt(y=audio, sr=sample_rate)
+        chroma_mean = chroma.mean(axis=1)
+        chroma_norm = chroma_mean / (np.linalg.norm(chroma_mean) + 1e-8)
+        scores_major = np.array([
+            self._compare_profile(chroma_norm, np.roll(_MAJOR_PROFILE, i)) for i in range(12)
+        ])
+        scores_minor = np.array([
+            self._compare_profile(chroma_norm, np.roll(_MINOR_PROFILE, i)) for i in range(12)
+        ])
+        best_major = int(np.argmax(scores_major))
+        best_minor = int(np.argmax(scores_minor))
+        if scores_major[best_major] >= scores_minor[best_minor]:
+            return _NOTE_NAMES[best_major], "major"
+        return _NOTE_NAMES[best_minor], "minor"
+
+    def _compare_profile(self, chroma_norm: np.ndarray, profile: np.ndarray) -> float:
+        profile_norm = profile / (np.linalg.norm(profile) + 1e-8)
+        return float(np.dot(chroma_norm, profile_norm))
+
+    def _estimate_energy(self, audio: np.ndarray) -> float:
+        loudness = self._meter.integrated_loudness(audio)
+        # Normalize loudness (typically -40 to 0 LUFS) into 0-10 scale
+        normalized = np.clip((loudness + 40.0) / 4.0, 0.0, 10.0)
+        return float(normalized)
+
+    def _detect_cues(self, audio: np.ndarray, sample_rate: int) -> List[CuePoint]:
+        onset_env = librosa.onset.onset_strength(y=audio, sr=sample_rate)
+        peaks = librosa.onset.onset_detect(onset_envelope=onset_env, sr=sample_rate, units="time")
+        cues: List[CuePoint] = []
+        if len(peaks) > 0:
+            cues.append(CuePoint(label="Intro", time_seconds=float(peaks[0])))
+        if len(peaks) > 1:
+            middle_index = len(peaks) // 2
+            cues.append(CuePoint(label="Drop", time_seconds=float(peaks[middle_index])))
+        if len(peaks) > 2:
+            cues.append(CuePoint(label="Outro", time_seconds=float(peaks[-1])))
+        if not cues:
+            cues.append(CuePoint(label="Start", time_seconds=0.0))
+        return cues
+
+
+def _extract_basic_tags(path: Path) -> tuple[str, str, str | None, str | None]:
+    """Extract common metadata using mutagen when available."""
+
+    try:
+        from mutagen import File
+    except ImportError:  # pragma: no cover - optional at runtime
+        return path.stem, "", None, None
+
+    metadata = File(path)
+    if metadata is None:
+        return path.stem, "", None, None
+
+    def _safe_get(keys: Sequence[str]) -> str:
+        for key in keys:
+            if key in metadata:
+                value = metadata[key]
+                if isinstance(value, list):
+                    return str(value[0])
+                return str(value)
+        return ""
+
+    title = _safe_get(["TIT2", "TITLE", "©nam"]) or path.stem
+    artist = _safe_get(["TPE1", "ARTIST", "©ART"])
+    album = _safe_get(["TALB", "ALBUM", "©alb"]) or None
+    genre = _safe_get(["TCON", "GENRE", "©gen"]) or None
+    return title, artist, album, genre
 
EOF
)
